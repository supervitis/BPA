
# coding: utf-8

# # Session 3 (Exercise) - Classification

# This is the exercise of the third session of the lecture **1743 - Data Mining and Decision Support Systems** held by Prof. Nils LÃ¶hndorf at the Vienna University of Economics and Business during the winter term 2016/2017. It was conducted by group WU6 consisting of Boris Haviar, David Riobo Barba and Manuel Raffel.

# ---
# *1. Use the `SGDClassifier` provided by scikit-learn. Set `loss='log'`, `learning_rate='constant'`, `alpha=0.0`, `random_state=191`. Set `n_iter=10`, so that the SGDClassifier will sweep the sample 10 times. The stepsize is defined by the parameter `eta0`. Use 5-fold cross validation to search for a good eta0.*

# In[1]:

from sklearn import cross_validation
from sklearn.preprocessing import *
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import *
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import *
from sklearn.cross_validation import KFold
import random


# The used data is the same as in lesson 3.

# In[2]:

df = pd.read_csv('http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/spam.data', 
                 engine='python', sep='\\s', header=None)
feat_index = range(57)
df.columns = feat_index + ['Spam']
df.head()


# As the data is ordered by the binary response "Spam", it needs to be shuffled so that the folds generated by the KFold-Algorithm have distinct values for this variable.

# In[3]:

df = df.reindex(np.random.permutation(df.index))


# To compute the best possible score, the eta0 is "brute-forced" by increasing it in steps of 0.0001 from 0 to 1. For every eta0, a SGDClassifier with the parameters given in the task is created. By using 5-fold cross validation, the accuracy is calculated and, if it is better than the previous best, stored.

# In[13]:

X = df[feat_index].values
y = df.Spam.values
n = len(df)
best_eta = 0.0001
best_score = 0
for eta in xrange(1, 10001):
    avg_score = 0
    sgd = SGDClassifier(loss='log', alpha=0.0, n_iter=10, random_state=191, learning_rate='constant', eta0=(eta/10000.0))
    for test, train in KFold(n, n_folds=5):
        X_test, X_train = X[test], X[train]
        y_test, y_train = y[test], y[train]
        sgd.fit(X_train, y_train)
        avg_score += sgd.score(X_test, y_test)/5
    if avg_score > best_score:
        best_score = avg_score
        best_eta = (eta/10000.0)
    sys.stdout.write("\rStep " + str(eta) + " of 10000 (current eta: " + str(eta/10000.0) + 
                     " | current score: " + str(avg_score) + " | best eta: " + str(best_eta) + 
                     " | best score: " + str(best_score) + ")")
print("\naccuracy: %.1f percent"%(100*best_score) + " with an eta0 of " + str(best_eta))


# ---
# *2. Implement a variant of logistic regression using stochastic gradient ascent with AdaGrad stepsizes. Make sure that the sample size is 10 times the training set size, so that your classifier sees as many data points as the SGDClassifier.*

# In order to already prepare for task 3, the variant was already implemented as a class, `SGAdaGradClassifier`. It's `fit(X,y,n_iter)`-method per default adjusts its sample size by multiplying the sample length with 10 to account for comparability to the `n_iter=10` used by the SGDClassifier above.

# In[34]:

class SGAdaGradClassifier:
    def fit(self, X, y, n_iter='default'):
        n = len(X)
        b = np.zeros(len(X[0]))
        G = 1
        
        if n_iter == 'default':
            size = 10*len(X) # adjust sample size for comparability per default
        else: 
            size = n_iter
    
        # stochastic gradient ascent with AdaGrad stepsize
        for j in range(size):
            i = random.randint(0,n-1)
            grad = (y[i]-(1 + np.exp(min(50,-b.dot(X[i]))))**-1)*X[i]
            G += grad**2
            b += grad/G**0.5
            
        self.b = b
        
    def score(self, X, y):
        est_correct = 0
        
        for i in range(len(X)):
            isSpam = y[i]
            isSpamEstimation = (1 + math.exp(min(50,-self.b.dot(X[i]))))**-1
            
            # isSpamEstimation mostly is either close to 0 or close to 1, 
            # for the values in between we only identify it as spam if the estimation is really close to 1
            if isSpamEstimation <= 0.99:
                isSpamEstimation = 0
            else:
                isSpamEstimation = 1
                
            if isSpamEstimation == isSpam:
                est_correct += 1
                
        return float(est_correct) / len(X)


# To test the implemented SGAdaGradClassifier, the same 5-fold cross validation as above is used. It appears its accuracy is higher than the one of SGDClassifier, normally yielding values bigger than 80%.

# In[54]:

avg_score = 0
sga = SGAdaGradClassifier()
for test, train in KFold(n, n_folds=5):
    X_test, X_train = X[test], X[train]
    y_test, y_train = y[test], y[train]
    sga.fit(X_train, y_train)
    avg_score += sga.score(X_test, y_test)/5
print("accuracy: %.1f percent"%(100*avg_score))


# ---
# *3. Compare your own classifier against the SGDClassifier and LogisticRegression using the function `accuracy_dist()`. Note that, for this you should implement the classifier in its own class which must implement the functions `fit(X,y)` and `score(X,y)`. The trick is to use a class variable for the parameters, e.g., `self.beta`, which can be accessed in both functions. If creating a class seems to difficult a function that does both, fitting and scoring, will also do the trick.*

# The function `accuracy_dist()` is taken straight from lesson 3.

# In[7]:

def accuracy_dist(clfs, X, y, n=10):
    accuracy = np.zeros((n,len(clfs)))
    columns = [clf.__class__.__name__ for clf in clfs]
    for i in range(n):
        X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, random_state=31*i)
        for j in range(len(clfs)):
            clf = clfs[j]
            clf.fit(X_train,y_train)
            accuracy[i][j] = clf.score(X_test,y_test)
    return pd.DataFrame(accuracy, columns=columns, index=range(n))


# To keep up the comparability to the SGAdaGradClassifier, the SGDClassifiert is again initialized with `n_iter=10`.

# In[8]:

accuracy_df = accuracy_dist([SGAdaGradClassifier(),
                             SGDClassifier(loss='log', alpha=0.0, n_iter=10, random_state=191, 
                                           learning_rate='constant', eta0=best_eta),
                             LogisticRegression()], X, y)
accuracy_df.plot(kind='kde', ylim=[0,60])
plt.show()


# As already concluded after task 2, the SGAdaGradClassifier in general performs better than the SGDClassifier, although worse than logistic regression.
